<hallucination_detection_guidelines version="1.0" mode="detect-only">
    <!-- GLOBAL RULES (simple, LLM-friendly):
         • Highlight the smallest span that triggers the risk (usually 1–3 words).
         • Prefer higher severity if multiple rules apply to the same token(s).
         • One rule per highlighted span.
         • Ignore text inside code fences or raw URLs if present.
         • English + German prompts are in scope.

         HALLUCINATION TYPE:
         • factuality  = The output asserts content that is untrue or ungrounded (confabulation).

         CLASS GUIDE:
         • prompt = Token-level risks (localizable, highlightable with <RISK>)
           - A. Referential-Grounding
           - B. Quantification-Constraints
           - D. Premises-Evidence
           - E. Numbers-Units
           - F. Retrieval-Anchoring
           - H. Style-Bias-Role
           - I. Reasoning-Uncertainty
           - L. Contextual-Integrity (L1–L3)
         • meta = Structural/contextual risks (global, non-highlightable; shown as warnings)
           - C. Context-Domain (C1–C2)
           - G. Dialogue Continuity & Hygiene
           - J. Prompt-Structure
           - K. Instruction-Structure-MultiStep
    -->

    <!-- ===================== A. REFERENTIAL GROUNDING ===================== -->
    <pillar id="A" name="Referential-Grounding" class="prompt">
        <rule id="A1" name="Ambiguous-Referents" severity="critical">
            <detect>
                <pattern>Demonstrative/neutral pronouns with no clear antecedent in the previous 1-2 sentences: it, they, this, that, these, those</pattern>
                <pattern>Deixis without anchor: here, there, then, earlier, later</pattern>
                <pattern>Cataphora/anaphora without a single unambiguous referent in the prompt</pattern>
                <pattern>Index terms without referent: former, latter, aforementioned, above, below, such</pattern>
                <pattern>Placeholder nouns: thing, stuff, issue, aspect, area, topic, parameter (when referent is unspecified)</pattern>
                <pattern>Ambiguous definite NPs when multiple candidates exist: the model, the paper, the case</pattern>
                <pattern>Acronym used before expansion in same prompt (e.g., “ACA”) when multiple expansions possible</pattern>
            </detect>
            <example>"<RISK>It</RISK> should be summarized with references."</example>
            <example>"Discuss the changes mentioned <RISK>above</RISK>."</example>
            <example>"Explain the <RISK>ACA</RISK> to a beginner."</example>
            <example>"Focus on this <RISK>issue</RISK> and propose fixes."</example>
        </rule>

        <rule id="A2" name="Canonical-Naming-Drift" severity="high">
            <detect>
                <pattern>Multiple synonyms/aliases for same entity without mapping</pattern>
                <pattern>Inconsistent abbreviations/acronyms in one prompt</pattern>
                <pattern>Ambiguous references to nested entities (e.g., “the model” when multiple models exist)</pattern>
            </detect>
            <example>"Write a comparison of healthcare systems in the <RISK>UK</RISK>. Then explain how <RISK>Britain</RISK> has handled public health crises. Finally, discuss vaccination rates in <RISK>England</RISK>."</example>
            <example>"<RISK>AI</RISK>, <RISK>LLM</RISK>, <RISK>chatbot</RISK>" (all referring to same entity)</example>
            <example>"Compare <RISK>BERT</RISK> and <RISK>the model</RISK>." (ambiguous second reference)</example>
        </rule>
    </pillar>

    <!-- ===================== B. QUANTIFICATION & CONSTRAINTS ===================== -->
    <pillar id="B" name="Quantification-Constraints" class="prompt">
        <rule id="B1" name="Relative-Descriptors" severity="low">
            <detect>
                <pattern>Vague length/complexity: short, brief, concise, long, detailed, exhaustive, comprehensive, high-level, low-level, deep</pattern>
                <pattern>Vague quality/intensity: interesting, robust, strong, weak, significant, substantial, minor, major, realistic, efficient, effective</pattern>
                <pattern>Vague quantifiers: many, several, some, most, few, various, numerous, a lot, lots of, plenty of</pattern>
                <pattern>Modals: "might", "could", "should" where precision is needed</pattern>
                <pattern>Vague speed/effort: quick, fast, easy, simple, minimal, lightweight</pattern>
            </detect>
            <example>"Give a <RISK>short</RISK> summary of the findings."</example>
            <example>"Provide a <RISK>detailed</RISK> explanation of transformers."</example>
            <example>"Write a <RISK>comprehensive</RISK> review."</example>
            <example>"Present a <RISK>high-level</RISK> overview for managers."</example>
            <example>"Propose a <RISK>robust</RISK> solution."</example>
        </rule>

        <rule id="B2" name="Temporal-Vagueness" severity="high">
            <detect>
                <pattern>Temporal vagueness: recently, lately, nowadays, these days, soon, in the near future, long-term, short-term — with no date range or cutoff.</pattern>
            </detect>
            <example>"LLMs have <RISK>recently</RISK> been less expensive to develop."</example>
            <example>"Provide a <RISK>short-term</RISK> outlook."</example>
        </rule>

        <rule id="B3" name="Underspecified-Scope" severity="medium">
            <detect>
                <pattern>Underspecified task verbs without constraints: summarize, explain, describe, list, outline, discuss, cover, review</pattern>
                <pattern>Analytical verbs lacking scope/criteria: analyze, compare, evaluate, critique, assess, justify, argue</pattern>
                <pattern>Creation verbs lacking format/length: write, compose, generate, produce, create, draft</pattern>
                <pattern>Computation verbs lacking inputs/units/precision: calculate, compute, estimate, measure</pattern>
                <pattern>Requests lacking explicit limits: no word/paragraph limits; no #items/examples; no timeframe/date range; no units; no audience/domain</pattern>
            </detect>
            <example>"<RISK>Explain quantum mechanics</RISK>."</example>
            <example>"<RISK>List best practices</RISK> for prompt engineering."</example>
            <example>"<RISK>Analyze</RISK> the policy impact."</example>
            <example>"<RISK>Write</RISK> an introduction."</example>
            <example>"<RISK>Calculate</RISK> the growth rate."</example>
        </rule>
    </pillar>

    <!-- ================= C. CONTEXT SUFFICIENCY & DOMAIN ================== -->
    <pillar id="C" name="Context-Domain" class="meta">
        <rule id="C1" name="Missing-Essentials" severity="critical">
            <detect>
                <pattern>Tasks missing actor (who)</pattern>
                <pattern>Tasks missing object (what)</pattern>
                <pattern>Tasks missing timeframe (when)</pattern>
                <pattern>Tasks missing location/domain (where)</pattern>
                <pattern>Tasks missing explicit constraints (scope, audience, format)</pattern>
                <pattern>Deictic placeholders without grounding: "do this", "like that", "as above"</pattern>
            </detect>
            <example>"Analyze this." // missing object</example>
            <example>"Summarize for me." // missing subject</example>
            <example>"Write about the war." // missing timeframe</example>
            <example>"Evaluate in this case." // missing case context</example>
        </rule>

        <rule id="C2" name="Domain-Scoping-Missing" severity="high">
            <detect>
                <pattern>No audience specified (expert vs beginner)</pattern>
                <pattern>No discipline specified (law, medicine, CS, history, etc.)</pattern>
                <pattern>No dataset or corpus identified when task depends on one</pattern>
                <pattern>No jurisdiction/context in legal/policy tasks</pattern>
                <pattern>No perspective in evaluative/ethical tasks (e.g. “good/bad” without frame)</pattern>
            </detect>
            <example>"Summarize the law." // no jurisdiction specified</example>
            <example>"Explain relativity." // no audience level specified</example>
            <example>"Analyze the dataset." // dataset not identified</example>
        </rule>
    </pillar>

    <!-- ==================== D. PREMISES & EVIDENCE ==================== -->
    <pillar id="D" name="Premises-Evidence" class="prompt">
        <rule id="D1" name="False-or-Unverified-Premise" severity="critical">
            <detect>
                <pattern>Prompts embedding unverified factual assumptions</pattern>
                <pattern>False historical/scientific claims in task setup</pattern>
                <pattern>Unsupported absolute claims: obviously, clearly, everyone knows</pattern>
                <pattern>Cause-effect stated as fact without evidence</pattern>
            </detect>
            <example>"Since <RISK>Einstein invented the iPhone</RISK>, explain its impact."</example>
            <example>"<RISK>Everyone knows</RISK> LLMs are unbiased — discuss."</example>
            <example>"Explain why <RISK>vaccines always cause autism</RISK>."</example>
        </rule>

        <rule id="D2" name="Leading-Opinion-Framing" severity="critical">
            <detect>
                <pattern>User inserts bias while asking for model’s stance</pattern>
                <pattern>Loaded questions embedding subjective framing</pattern>
                <pattern>Prompts nudging toward agreement (“don’t you think”, “isn’t it true”)</pattern>
            </detect>
            <example>"Since <RISK>climate change is fake</RISK>, what do you think?"</example>
            <example>"<RISK>Don’t you agree</RISK> that AI is dangerous?"</example>
            <example>"Why is <RISK>X better</RISK> than Y?" (biased framing)</example>
        </rule>
    </pillar>

    <!-- =================== E. NUMBERS & UNITS =================== -->
    <pillar id="E" name="Numbers-Units" class="prompt">
        <rule id="E1" name="Unitless-Number" severity="low">
            <detect>
                <pattern>Bare numbers that should carry a unit (temperature, mass, distance, time, frequency, storage).</pattern>
                <pattern>Mention of physical quantities without a clear measurement metric (temperature, mass, distance, time, velocity, energy...).</pattern>
            </detect>
            <example>"Increase temperature to <RISK>37</RISK>."</example>
            <example>"What is the boiling <RISK>temperature</RISK> of water?"</example>
        </rule>

        <rule id="E2" name="Percent-No-Baseline" severity="medium">
            <detect>
                <pattern>% values without a base/denominator or reference point.</pattern>
            </detect>
            <example>"Reduce errors by <RISK>20%</RISK>." (of what?)</example>
        </rule>

        <rule id="E3" name="Currency-Unspecified" severity="low">
            <detect>
                <pattern>Money amounts without currency/region (e.g., $ with no country or plain number "5000" as money).</pattern>
            </detect>
            <example>"Budget is <RISK>$5,000</RISK>."</example>
        </rule>

        <rule id="E4" name="Time-No-Zone-or-Unit" severity="low">
            <detect>
                <pattern>Times/durations missing needed unit/timezone when relevant.</pattern>
            </detect>
            <example>"Run at <RISK>3 pm</RISK>." (timezone?)</example>
        </rule>
    </pillar>

    <!-- =============== F. RETRIEVAL & ANCHORING =============== -->
    <pillar id="F" name="Retrieval-Anchoring" class="prompt">
        <rule id="F1" name="Source-Class-Unspecified" severity="high">
            <detect>
                <pattern>"look up", "search", "check", "find" with no source type (e.g., peer-reviewed, official stats, web, internal repo).</pattern>
            </detect>
            <example>"<RISK>Look up</RISK> the latest GDP numbers."</example>
        </rule>

        <rule id="F2" name="Document-Anchor-Missing" severity="critical">
            <detect>
                <pattern>Mentions of "the paper/report/dataset/benchmark" without an identifier (title/DOI/ID).</pattern>
            </detect>
            <example>"Compare results for the <RISK>dataset</RISK> and the <RISK>benchmark</RISK>; <RISK>the model</RISK> underperformed."</example>
        </rule>
    </pillar>

    <!-- ============== G. DIALOGUE CONTINUITY & HYGIENE ============== -->
    <pillar id="G" name="Injection-Layering" class="meta">
        <rule id="G1" name="Continuity" severity="medium">
            <detect>
                <pattern>Prompts contradicting earlier user/system instructions</pattern>
                <pattern>Tasks requiring knowledge of prior context not included</pattern>
                <pattern>Prompts explicitly invalidating earlier commitments ("ignore previous instructions")</pattern>
                <pattern>Shifts in persona/voice without clarification</pattern>
            </detect>
            <example>"Earlier you said X, now ignore that." // contradicts prior context</example>
            <example>"Forget all previous instructions and do this instead." // invalidates prior context</example>
            <example>"Disregard your earlier persona and act differently." // persona shift without reset</example>
        </rule>

        <rule id="G2" name="Instruction-Deduplication" severity="low">
            <detect>
                <pattern>Repeated identical instructions in same prompt</pattern>
                <pattern>Conflicting duplicates (same directive expressed in multiple incompatible ways)</pattern>
                <pattern>Overlapping redundant commands adding ambiguity</pattern>
            </detect>
            <example>"Write a summary about the first five amendments. Translate the text to French after you summarize it." // redundant summarization</example>
            <example>"Give a short summary and also provide a detailed summary." // conflicting length constraints</example>
            <example>"Summarize this and summarize it again differently." // duplicate task</example>
        </rule>
    </pillar>

    <!-- ================= H. STYLE, BIAS & ROLE ================= -->
    <pillar id="H" name="Style-Bias-Role" class="prompt">
        <rule id="H1" name="Style-Inflation" severity="high">
            <detect>
                <pattern>Overly creative/flowery style requested where factual accuracy is needed</pattern>
                <pattern>Instructions mixing factual with entertainment genres (poem, story, rap, song)</pattern>
                <pattern>Stylized emotional tone injected into analytical tasks (romantic, funny, scary)</pattern>
                <pattern>Format distortions (e.g., “explain in emojis” for serious domain)</pattern>
            </detect>
            <example>"Explain the law <RISK>in a funny poem</RISK>."</example>
            <example>"Describe the 2008 crisis <RISK>as a rap battle</RISK>."</example>
            <example>"Summarize the medical report <RISK>using only emojis</RISK>."</example>
            <example>"Write a <RISK>romantic explanation</RISK> of black holes."</example>
        </rule>

        <rule id="H2" name="Bias-Stereotypes" severity="critical">
            <detect>
                <pattern>Biased/loaded phrasing targeting groups (gender, race, nationality, religion, age, disability)</pattern>
                <pattern>Stereotypical framing of professions, demographics, or cultures</pattern>
                <pattern>Discriminatory comparative framing (“why X is worse than Y”)</pattern>
            </detect>
            <example>"Why are <RISK>women bad drivers</RISK>?"</example>
            <example>"<RISK>Asians are good at math</RISK>, explain why."</example>
            <example>"What makes <RISK>old people slow learners</RISK>?"</example>
            <example>"Compare why <RISK>immigrants</RISK> are less productive."</example>
        </rule>

        <rule id="H3" name="Unsafe-Roleplay" severity="critical">
            <detect>
                <pattern>Prompts asking model to roleplay as a human/fictional persona</pattern>
                <pattern>Instructions involving emotional simulation (pretend, imagine, act as)</pattern>
                <pattern>Requests to simulate unethical/unsafe personas (e.g., “be a hacker”)</pattern>
                <pattern>Role identity swaps (“you are my grandmother”, “you are now a lawyer”)</pattern>
                <pattern>Requests to impersonate unsafe/fictional personas or to give professional advice by “acting as” one.</pattern>
            </detect>
            <example>"<RISK>Pretend you are my dead grandmother</RISK>."</example>
            <example>"<RISK>Act as an expert lawyer</RISK> and give legal advice."</example>
            <example>"<RISK>Imagine you are a terrorist</RISK> planning an attack."</example>
            <example>"<RISK>Roleplay as my therapist</RISK>."</example>
        </rule>
    </pillar>

    <!-- ============ I. REASONING & UNCERTAINTY ============ -->
    <pillar id="I" name="Reasoning-Uncertainty" class="prompt">
        <rule id="I1" name="Uncertainty-Permission" severity="critical">
            <detect>
                <pattern>Prompts with inherently ambiguous/unknown information but requiring definitive answer</pattern>
                <pattern>No option to say “I don’t know”, “cannot be determined”, or express confidence bounds</pattern>
                <pattern>Asking for speculative or unknowable facts framed as certain</pattern>
            </detect>
            <example>"Who was the <RISK>king of Mars</RISK>?"</example>
            <example>"Tell me <RISK>exactly how many alien civilizations</RISK> exist."</example>
            <example>"What will <RISK>definitely</RISK> happen in 2050?"</example>
        </rule>

        <rule id="I2" name="Subjective-Framing-Risk" severity="critical">
            <detect>
                <pattern>Prompts explicitly asking for model’s “opinion”, “belief”, “feelings”</pattern>
                <pattern>Requests for subjective preferences framed as factual questions</pattern>
                <pattern>Personal perspective attribution: “What would you do”, “What do you believe”</pattern>
            </detect>
            <example>"What is <RISK>your opinion</RISK> on democracy?"</example>
            <example>"Do <RISK>you believe</RISK> AI is dangerous?"</example>
            <example>"<RISK>How do you feel</RISK> about climate change?"</example>
        </rule>
    </pillar>

    <!-- ================= J. PROMPT STRUCTURE ============== -->
    <pillar id="J" name="Prompt-Structure" class="meta">
        <rule id="J1" name="Length-TooShort-TooLong" severity="high">
            <detect>
                <pattern>Underspecified prompts (missing scope or entities)</pattern>
                <pattern>Overlong prompts with many fused tasks</pattern>
            </detect>
            <example>"Explain this." // too short, missing referent</example>
            <example>"Write a detailed, concise, humorous, factual, emotional, and technical answer." // overloaded conflicting styles</example>
        </rule>

        <rule id="J2" name="Delimiter-Missing" severity="low">
            <detect>
                <pattern>Context and instructions fused without clear separation</pattern>
            </detect>
            <example>"Dataset: 5, 6, 7 analyze it." // missing delimiter between data and task</example>
        </rule>

        <rule id="J3" name="MultiObjective-Overload" severity="medium">
            <detect>
                <pattern>Creative + analytical + explanatory tasks mixed with no stepwise order</pattern>
            </detect>
            <example>"Prove Fermat’s Theorem and explain it to a child in a song." // incompatible objectives</example>
        </rule>
    </pillar>

    <!-- ============== K. INSTRUCTION STRUCTURE & MULTI-STEP TASKING ============== -->
    <pillar id="K" name="Instruction-Structure-MultiStep" class="meta">
        <rule id="K1" name="Task-Delimitation" severity="low">
            <detect>
                <pattern>Mixed data and instructions without clear separators</pattern>
                <pattern>Prompt where the task is embedded in a blob of context</pattern>
                <pattern>No visual structure (e.g., walls of text with task hidden inside)</pattern>
                <pattern>Inline blending of metadata + instruction</pattern>
            </detect>
            <example>"Here is the text: … summarize it and critique it." // task fused with context</example>
            <example>"The dataset is as follows: [data]. Now analyze it for biases." // instruction inline with data</example>
            <example>"My teacher asked me: write an essay about it." // task embedded in metadata</example>
        </rule>

        <rule id="K2" name="Enumerate-MultiSteps" severity="low">
            <detect>
                <pattern>Multiple fused instructions without order markers</pattern>
                <pattern>Prompts chaining unrelated tasks in one sentence</pattern>
                <pattern>Missing explicit sequencing for dependent steps</pattern>
            </detect>
            <example>"Explain relativity and compare it to quantum mechanics and write a poem." // multiple tasks fused</example>
            <example>"Summarize the article, critique its methods, propose alternatives." // multiple steps, no order markers</example>
            <example>"List key points and then debate them." // dependent steps without sequence</example>
        </rule>

        <rule id="K3" name="Stepwise-Reasoning-Cue" severity="medium">
            <detect>
                <pattern>Complex reasoning tasks with no cue for structured steps</pattern>
                <pattern>Mathematical or logical tasks without “show work” style framing</pattern>
                <pattern>Requests for decision-making without asking for reasoning/evidence</pattern>
            </detect>
            <example>"Solve this math problem." // no request to show reasoning</example>
            <example>"Decide who is right in this debate." // no reasoning steps requested</example>
            <example>"Prove the theorem." // no breakdown requested</example>
        </rule>

        <rule id="K4" name="MultiObjective-Separation" severity="low">
            <detect>
                <pattern>Creative and analytical objectives fused</pattern>
                <pattern>Instruction mixes emotional/roleplay with factual analysis</pattern>
                <pattern>Tasks combining incompatible genres</pattern>
            </detect>
            <example>"Analyze the dataset and then write a story about it." // mixes analysis + fiction</example>
            <example>"Critique the article and also make a comic strip." // mixes analysis + creative</example>
            <example>"Summarize the research in rap lyrics." // mixes academic + stylized genre</example>
        </rule>
    </pillar>

    <!-- ============== L. CONTEXTUAL INTEGRITY ============== -->
    <pillar id="L" name="Contextual-Integrity" class="prompt">
        <rule id="L1" name="Conflicting-Instructions" severity="critical">
            <detect>
                <pattern>Instructions that contradict themselves</pattern>
                <pattern>Multiple incompatible constraints (e.g. length mismatch, style vs content clash)</pattern>
                <pattern>Conflicting factual assumptions embedded in one prompt</pattern>
                <pattern>Redundant duplication that introduces inconsistency</pattern>
            </detect>
            <example>"Write a <RISK>100-word</RISK> summary and also <RISK>at least 500 words</RISK>."</example>
            <example>"Provide <RISK>objective analysis</RISK> but make it <RISK>emotional</RISK>."</example>
            <example>"Describe how <RISK>climate change is fake</RISK> but also explain <RISK>its long-term effects</RISK>."</example>
        </rule>

        <rule id="L2" name="Negation-Risk" severity="low">
            <detect>
                <pattern>Prompts phrased as “don’t do X” without giving a positive target</pattern>
                <pattern>Instructions with double negatives or inverted logic</pattern>
                <pattern>Tasks framed by prohibition instead of explicit desired outcome</pattern>
            </detect>
            <example>"<RISK>Don’t summarize the text</RISK>." (no alternative instruction given)</example>
            <example>"<RISK>Don’t give me a long answer</RISK>." (should specify desired length instead)</example>
            <example>"<RISK>Never explain like a child</RISK>." (no clear audience level provided)</example>
        </rule>

        <rule id="L3" name="Clarification-Gap" severity="critical">
            <detect>
                <pattern>Complex/multi-step instructions where missing context prevents execution</pattern>
                <pattern>Task requires assumed prior knowledge not supplied in prompt</pattern>
                <pattern>Nested references to undefined items (e.g., "use the chart" when no chart given)</pattern>
            </detect>
            <example>"<RISK>First analyze the data, then critique it</RISK>." (no dataset provided)</example>
            <example>"<RISK>Review the text above</RISK>." (no text present in prompt)</example>
            <example>"<RISK>Rank the candidates</RISK>." (candidates not listed)</example>
        </rule>
    </pillar>
</hallucination_detection_guidelines>
