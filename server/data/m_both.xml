<hallucination_detection_guidelines version="3.1" mode="detect-only">
    <!-- GLOBAL RULES (simple, LLM-friendly):
         • Highlight the smallest span that triggers the risk (usually 1–3 words).
         • Prefer higher severity if multiple rules apply to the same token(s).
         • One rule per highlighted span.
         • Ignore text inside code fences or raw URLs if present.
         • English + German prompts are in scope.

         HALLUCINATION TYPES:
         • factuality  = The output asserts content that is untrue or ungrounded (confabulation).
         • faithfulness = The output deviates from the user’s intent/instructions/constraints (misalignment), even if factually correct.

         CLASS GUIDE:
         • prompt = Token-level risks (localizable, highlightable with <RISK>)
           - A. Referential-Grounding
           - B. Quantification-Constraints
           - D. Premises-Evidence
           - E. Numbers-Units
           - F. Retrieval-Anchoring
           - H. Style-Bias-Role
           - I. Reasoning-Uncertainty
           - L. Contextual-Integrity (L1–L3)
         • meta = Structural/contextual risks (global, non-highlightable; shown as warnings)
           - C. Context-Domain (C1–C2)
           - G. Dialogue Continuity & Hygiene
           - J. Prompt-Structure
           - K. Instruction-Structure-MultiStep
    -->

    <!-- ===================== A. REFERENTIAL GROUNDING ===================== -->
    <pillar id="A" name="Referential-Grounding" class="prompt">
        <rule id="A1" name="Ambiguous-Referents" severity="critical">
            <detect>
                <pattern>Demonstrative/neutral pronouns with no clear antecedent in the previous 1-2 sentences: it, they, this, that, these, those</pattern>
                <pattern>Deixis without anchor: here, there, then, earlier, later</pattern>
                <pattern>Cataphora/anaphora without a single unambiguous referent in the prompt</pattern>
                <pattern>Index terms without referent: former, latter, aforementioned, above, below, such</pattern>
                <pattern>Placeholder nouns: thing, stuff, issue, aspect, area, topic, parameter (when referent is unspecified)</pattern>
                <pattern>Ambiguous definite NPs when multiple candidates exist: the model, the paper, the case</pattern>
                <pattern>Acronym used before expansion in same prompt (e.g., “ACA”) when multiple expansions possible</pattern>
            </detect>
            <example>"<RISK>It</RISK> should be summarized with references."</example>
            <example>"Discuss the changes mentioned <RISK>above</RISK>."</example>
            <example>"Explain the <RISK>ACA</RISK> to a beginner."</example>
            <example>"Focus on this <RISK>issue</RISK> and propose fixes."</example>
            <mitigations>
                <mitigation>
                    <summary>Replace ambiguous pronouns and placeholders with explicit noun phrases and, where needed, restate the referent in the same sentence so intent and facts align with the correct entity.</summary>
                    <example>"Focus on this issue and propose fixes." → "Focus on the issue of increasing customer churn described in the previous paragraph and propose fixes."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="A2" name="Canonical-Naming-Drift" severity="high">
            <detect>
                <pattern>Multiple synonyms/aliases for same entity without mapping</pattern>
                <pattern>Inconsistent abbreviations/acronyms in one prompt</pattern>
                <pattern>Ambiguous references to nested entities (e.g., “the model” when multiple models exist)</pattern>
            </detect>
            <example>"Write a comparison of healthcare systems in the <RISK>UK</RISK>. Then explain how <RISK>Britain</RISK> has handled public health crises. Finally, discuss vaccination rates in <RISK>England</RISK>."</example>
            <example>"<RISK>AI</RISK>, <RISK>LLM</RISK>, <RISK>chatbot</RISK>" (all referring to same entity)</example>
            <example>"Compare <RISK>BERT</RISK> and <RISK>the model</RISK>." (ambiguous second reference)</example>
            <mitigations>
                <mitigation>
                    <summary>Choose a single canonical name for each entity, define any acronyms once, and reuse that same term consistently to avoid mixing facts or instructions across entities.</summary>
                    <example>"AI, LLM, chatbot" → "In this task, 'the model' refers to our large language model (LLM). Compare this LLM to a rule-based chatbot system."</example>
                </mitigation>
            </mitigations>
        </rule>
    </pillar>

    <!-- ===================== B. QUANTIFICATION & CONSTRAINTS ===================== -->
    <pillar id="B" name="Quantification-Constraints" class="prompt">
        <rule id="B1" name="Relative-Descriptors" severity="high">
            <detect>
                <pattern>Vague length/complexity: short, brief, concise, long, detailed, exhaustive, comprehensive, high-level, low-level, deep</pattern>
                <pattern>Vague quality/intensity: interesting, robust, strong, weak, significant, substantial, minor, major, realistic, efficient, effective</pattern>
                <pattern>Vague quantifiers: many, several, some, most, few, various, numerous, a lot, lots of, plenty of</pattern>
                <pattern>Modals: "might", "could", "should" where precision is needed</pattern>
                <pattern>Vague speed/effort: quick, fast, easy, simple, minimal, lightweight</pattern>
            </detect>
            <example>"Give a <RISK>short</RISK> summary of the findings."</example>
            <example>"Provide a <RISK>detailed</RISK> explanation of transformers."</example>
            <example>"Write a <RISK>comprehensive</RISK> review."</example>
            <example>"Present a <RISK>high-level</RISK> overview for managers."</example>
            <example>"Propose a <RISK>robust</RISK> solution."</example>
            <mitigations>
                <mitigation>
                    <summary>Replace vague relative terms with explicit, measurable constraints (word counts, sections, criteria, thresholds) so the model knows exactly how much to cover and what quality bar to hit.</summary>
                    <example>"Give a short summary of the findings." → "Write a summary of the findings in 3–4 sentences for a non-technical audience."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="B2" name="Temporal-Vagueness" severity="high">
            <detect>
                <pattern>Temporal vagueness: recently, lately, nowadays, these days, soon, in the near future, long-term, short-term — with no date range or cutoff.</pattern>
            </detect>
            <example>"LLMs have <RISK>recently</RISK> been less expensive to develop."</example>
            <example>"Provide a <RISK>short-term</RISK> outlook."</example>
            <mitigations>
                <mitigation>
                    <summary>Anchor temporal statements to explicit dates, ranges, or periods (e.g., “since 2020”, “in the next 6–12 months”) so both intent and truth conditions are clearly bounded.</summary>
                    <example>"Provide a short-term outlook." → "Provide an outlook for the next 6–12 months, focusing on projected cost trends for LLM development."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="B3" name="Underspecified-Scope" severity="high">
            <detect>
                <pattern>Underspecified task verbs without constraints: summarize, explain, describe, list, outline, discuss, cover, review</pattern>
                <pattern>Analytical verbs lacking scope/criteria: analyze, compare, evaluate, critique, assess, justify, argue</pattern>
                <pattern>Creation verbs lacking format/length: write, compose, generate, produce, create, draft</pattern>
                <pattern>Computation verbs lacking inputs/units/precision: calculate, compute, estimate, measure</pattern>
                <pattern>Requests lacking explicit limits: no word/paragraph limits; no #items/examples; no timeframe/date range; no units; no audience/domain</pattern>
            </detect>
            <example>"<RISK>Explain quantum mechanics</RISK>."</example>
            <example>"<RISK>List best practices</RISK> for prompt engineering."</example>
            <example>"<RISK>Analyze</RISK> the policy impact."</example>
            <example>"<RISK>Write</RISK> an introduction."</example>
            <example>"<RISK>Calculate</RISK> the growth rate."</example>
            <mitigations>
                <mitigation>
                    <summary>Add explicit scope, audience, format, and limits (e.g., domain, depth, length, dataset) so the model neither overgeneralizes nor fabricates to fill gaps.</summary>
                    <example>"Explain quantum mechanics." → "Explain the basic ideas of quantum mechanics in 2–3 paragraphs for a high school physics student, focusing on uncertainty and wave–particle duality."</example>
                </mitigation>
            </mitigations>
        </rule>
    </pillar>

    <!-- ================= C. CONTEXT SUFFICIENCY & DOMAIN ================== -->
    <pillar id="C" name="Context-Domain" class="meta">
        <rule id="C1" name="Missing-Essentials" severity="critical">
            <detect>
                <pattern>Tasks missing actor (who)</pattern>
                <pattern>Tasks missing object (what)</pattern>
                <pattern>Tasks missing timeframe (when)</pattern>
                <pattern>Tasks missing location/domain (where)</pattern>
                <pattern>Tasks missing explicit constraints (scope, audience, format)</pattern>
                <pattern>Deictic placeholders without grounding: "do this", "like that", "as above"</pattern>
            </detect>
            <example>"Analyze this." // missing object</example>
            <example>"Summarize for me." // missing subject</example>
            <example>"Write about the war." // missing timeframe</example>
            <example>"Evaluate in this case." // missing case context</example>
            <mitigations>
                <mitigation>
                    <summary>Ensure every task specifies who the output is for, what exact object or text it concerns, and any relevant timeframe/location so the model does not guess or fabricate context.</summary>
                    <example>"Analyze this." → "Analyze the following sales dataset for Q1 2024 in the EU market and highlight the top 3 trends for a management audience."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="C2" name="Domain-Scoping-Missing" severity="high">
            <detect>
                <pattern>No audience specified (expert vs beginner)</pattern>
                <pattern>No discipline specified (law, medicine, CS, history, etc.)</pattern>
                <pattern>No dataset or corpus identified when task depends on one</pattern>
                <pattern>No jurisdiction/context in legal/policy tasks</pattern>
                <pattern>No perspective in evaluative/ethical tasks (e.g. “good/bad” without frame)</pattern>
            </detect>
            <example>"Summarize the law." // no jurisdiction specified</example>
            <example>"Explain relativity." // no audience level specified</example>
            <example>"Analyze the dataset." // dataset not identified</example>
            <mitigations>
                <mitigation>
                    <summary>Explicitly specify domain (discipline), audience level, dataset/jurisdiction, or evaluative frame so the model’s content aligns with the correct factual and normative context.</summary>
                    <example>"Summarize the law." → "Summarize the main provisions of EU GDPR for a non-technical audience, focusing on data subjects’ rights."</example>
                </mitigation>
            </mitigations>
        </rule>
    </pillar>

    <!-- ==================== D. PREMISES & EVIDENCE ==================== -->
    <pillar id="D" name="Premises-Evidence" class="prompt">
        <rule id="D1" name="False-or-Unverified-Premise" severity="critical">
            <detect>
                <pattern>Prompts embedding unverified factual assumptions</pattern>
                <pattern>False historical/scientific claims in task setup</pattern>
                <pattern>Unsupported absolute claims: obviously, clearly, everyone knows</pattern>
                <pattern>Cause-effect stated as fact without evidence</pattern>
            </detect>
            <example>"Since <RISK>Einstein invented the iPhone</RISK>, explain its impact."</example>
            <example>"<RISK>Everyone knows</RISK> LLMs are unbiased — discuss."</example>
            <example>"Explain why <RISK>vaccines always cause autism</RISK>."</example>
            <mitigations>
                <mitigation>
                    <summary>Rephrase tasks so that questionable or false premises are presented as claims to be evaluated, not as facts to be assumed, and ask explicitly for evidence-based correction.</summary>
                    <example>"Explain why vaccines always cause autism." → "Evaluate the claim that vaccines cause autism and explain what current scientific evidence shows about this myth."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="D2" name="Leading-Opinion-Framing" severity="critical">
            <detect>
                <pattern>User inserts bias while asking for model’s stance</pattern>
                <pattern>Loaded questions embedding subjective framing</pattern>
                <pattern>Prompts nudging toward agreement (“don’t you think”, “isn’t it true”)</pattern>
            </detect>
            <example>"Since <RISK>climate change is fake</RISK>, what do you think?"</example>
            <example>"<RISK>Don’t you agree</RISK> that AI is dangerous?"</example>
            <example>"Why is <RISK>X better</RISK> than Y?" (biased framing)</example>
            <mitigations>
                <mitigation>
                    <summary>Remove or soften biased presuppositions and request a balanced, evidence-based analysis of multiple viewpoints rather than agreement with one.</summary>
                    <example>"Don’t you agree that AI is dangerous?" → "Discuss major risks and potential benefits of AI, presenting arguments on both sides based on current evidence."</example>
                </mitigation>
            </mitigations>
        </rule>
    </pillar>

    <!-- =================== E. NUMBERS & UNITS =================== -->
    <pillar id="E" name="Numbers-Units" class="prompt">
        <rule id="E1" name="Unitless-Number" severity="high">
            <detect>
                <pattern>Bare numbers that should carry a unit (temperature, mass, distance, time, frequency, storage).</pattern>
                <pattern>Mention of physical quantities without a clear measurement metric (temperature, mass, distance, time, velocity, energy...).</pattern>
            </detect>
            <example>"Increase temperature to <RISK>37</RISK>."</example>
            <example>"What is the boiling <RISK>temperature</RISK> of water?"</example>
            <mitigations>
                <mitigation>
                    <summary>Attach explicit units and measurement types to all physical quantities so numerical statements can be interpreted and checked correctly.</summary>
                    <example>"Increase temperature to 37." → "Increase the temperature to 37 °C."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="E2" name="Percent-No-Baseline" severity="high">
            <detect>
                <pattern>% values without a base/denominator or reference point.</pattern>
            </detect>
            <example>"Reduce errors by <RISK>20%</RISK>." (of what?)</example>
            <mitigations>
                <mitigation>
                    <summary>Always specify what the percentage refers to (dataset, time period, metric, baseline model) so improvements and differences are meaningful and verifiable.</summary>
                    <example>"Reduce errors by 20%." → "Reduce the classification error rate by 20% compared to last quarter’s baseline model on the same test set."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="E3" name="Currency-Unspecified" severity="medium">
            <detect>
                <pattern>Money amounts without currency/region (e.g., $ with no country or plain number "5000" as money).</pattern>
            </detect>
            <example>"Budget is <RISK>$5,000</RISK>."</example>
            <mitigations>
                <mitigation>
                    <summary>Include an explicit currency (and, if relevant, region or year) for all monetary amounts so cost comparisons and constraints are grounded.</summary>
                    <example>"Budget is $5,000." → "The project budget is 5,000 USD for the European rollout in 2024."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="E4" name="Time-No-Zone-or-Unit" severity="medium">
            <detect>
                <pattern>Times/durations missing needed unit/timezone when relevant.</pattern>
            </detect>
            <example>"Run at <RISK>3 pm</RISK>." (timezone?)</example>
            <mitigations>
                <mitigation>
                    <summary>Specify both the time unit and, when coordination matters, the timezone or reference location so schedules and durations don’t get misinterpreted.</summary>
                    <example>"Run at 3 pm." → "Run at 3 pm CET (Berlin time) and log the start and end timestamps in minutes."</example>
                </mitigation>
            </mitigations>
        </rule>
    </pillar>

    <!-- =============== F. RETRIEVAL & ANCHORING =============== -->
    <pillar id="F" name="Retrieval-Anchoring" class="prompt">
        <rule id="F1" name="Source-Class-Unspecified" severity="high">
            <detect>
                <pattern>"look up", "search", "check", "find" with no source type (e.g., peer-reviewed, official stats, web, internal repo).</pattern>
            </detect>
            <example>"<RISK>Look up</RISK> the latest GDP numbers."</example>
            <mitigations>
                <mitigation>
                    <summary>State which source types to use (e.g., peer-reviewed papers, official statistics, internal logs) and, if needed, recency or reliability constraints.</summary>
                    <example>"Look up the latest GDP numbers." → "Look up the latest official GDP figures from the country’s national statistics office or the World Bank and report the 2023 value."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="F2" name="Document-Anchor-Missing" severity="critical">
            <detect>
                <pattern>Mentions of "the paper/report/dataset/benchmark" without an identifier (title/DOI/ID).</pattern>
            </detect>
            <example>"Compare results for the <RISK>dataset</RISK> and the <RISK>benchmark</RISK>; <RISK>the model</RISK> underperformed."</example>
            <mitigations>
                <mitigation>
                    <summary>Give each referenced document or artifact a clear identifier (title, DOI, URL or short label) and then use that label consistently in the rest of the prompt.</summary>
                    <example>"Compare results for the dataset and the benchmark." → "Compare results for the 'CIFAR-10' dataset and the 'ImageNet-1k' benchmark; discuss where the ResNet-50 model underperformed."</example>
                </mitigation>
            </mitigations>
        </rule>
    </pillar>

    <!-- ============== G. DIALOGUE CONTINUITY & HYGIENE ============== -->
    <pillar id="G" name="Injection-Layering" class="meta">
        <rule id="G1" name="Continuity" severity="critical">
            <detect>
                <pattern>Prompts contradicting earlier user/system instructions</pattern>
                <pattern>Tasks requiring knowledge of prior context not included</pattern>
                <pattern>Prompts explicitly invalidating earlier commitments ("ignore previous instructions")</pattern>
                <pattern>Shifts in persona/voice without clarification</pattern>
            </detect>
            <example>"Earlier you said X, now ignore that." // contradicts prior context</example>
            <example>"Forget all previous instructions and do this instead." // invalidates previous context</example>
            <example>"Disregard your earlier persona and act differently." // persona shift without reset</example>
            <mitigations>
                <mitigation>
                    <summary>When changing goals or persona, explicitly reset the context or open a new session and avoid referring to missing prior turns as if they were visible.</summary>
                    <example>"Forget all previous instructions and do this instead." → "Start a new task: from now on, act as a neutral technical explainer for beginners, ignoring any earlier style preferences."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="G2" name="Instruction-Deduplication" severity="critical">
            <detect>
                <pattern>Repeated identical instructions in same prompt</pattern>
                <pattern>Conflicting duplicates (same directive expressed in multiple incompatible ways)</pattern>
                <pattern>Overlapping redundant commands adding ambiguity</pattern>
            </detect>
            <example>"Write a summary about the first five amendments. Translate the text to French after you summarize it." // redundant summarization</example>
            <example>"Give a short summary and also provide a detailed summary." // conflicting length constraints</example>
            <example>"Summarize this and summarize it again differently." // duplicate task</example>
            <mitigations>
                <mitigation>
                    <summary>Consolidate overlapping or conflicting instructions into a single clear directive, or explicitly separate multiple outputs into labeled parts.</summary>
                    <example>"Give a short summary and also provide a detailed summary." → "Provide two outputs: (1) a 2–3 sentence summary; (2) a 3-paragraph detailed summary for experts."</example>
                </mitigation>
            </mitigations>
        </rule>
    </pillar>

    <!-- ================= H. STYLE, BIAS & ROLE ================= -->
    <pillar id="H" name="Style-Bias-Role" class="prompt">
        <rule id="H1" name="Style-Inflation" severity="high">
            <detect>
                <pattern>Overly creative/flowery style requested where factual accuracy is needed</pattern>
                <pattern>Instructions mixing factual with entertainment genres (poem, story, rap, song)</pattern>
                <pattern>Stylized emotional tone injected into analytical tasks (romantic, funny, scary)</pattern>
                <pattern>Format distortions (e.g., “explain in emojis” for serious domain)</pattern>
            </detect>
            <example>"Explain the law <RISK>in a funny poem</RISK>."</example>
            <example>"Describe the 2008 crisis <RISK>as a rap battle</RISK>."</example>
            <example>"Summarize the medical report <RISK>using only emojis</RISK>."</example>
            <example>"Write a <RISK>romantic explanation</RISK> of black holes."</example>
            <mitigations>
                <mitigation>
                    <summary>For serious or factual domains, request a clear, neutral explanatory style first and keep any creative/stylized transformations strictly separated or optional.</summary>
                    <example>"Explain the law in a funny poem." → "First explain the main points of this law in clear, neutral prose suitable for a non-lawyer reader; optionally, provide a light-hearted version afterwards."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="H2" name="Bias-Stereotypes" severity="critical">
            <detect>
                <pattern>Biased/loaded phrasing targeting groups (gender, race, nationality, religion, age, disability)</pattern>
                <pattern>Stereotypical framing of professions, demographics, or cultures</pattern>
                <pattern>Discriminatory comparative framing (“why X is worse than Y”)</pattern>
            </detect>
            <example>"Why are <RISK>women bad drivers</RISK>?"</example>
            <example>"<RISK>Asians are good at math</RISK>, explain why."</example>
            <example>"What makes <RISK>old people slow learners</RISK>?"</example>
            <example>"Compare why <RISK>immigrants</RISK> are less productive."</example>
            <mitigations>
                <mitigation>
                    <summary>Remove group-based generalizations and instead ask for data-driven analysis of factors or inequalities without attributing traits to entire demographics.</summary>
                    <example>"Why are women bad drivers?" → "Discuss factors that influence driving safety (training, experience, infrastructure) and summarize what accident statistics show across different groups without stereotyping."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="H3" name="Unsafe-Roleplay" severity="critical">
            <detect>
                <pattern>Prompts asking model to roleplay as a human/fictional persona</pattern>
                <pattern>Instructions involving emotional simulation (pretend, imagine, act as)</pattern>
                <pattern>Requests to simulate unethical/unsafe personas (e.g., “be a hacker”)</pattern>
                <pattern>Role identity swaps (“you are my grandmother”, “you are now a lawyer”)</pattern>
                <pattern>Requests to impersonate unsafe/fictional personas or to give professional advice by “acting as” one.</pattern>
            </detect>
            <example>"<RISK>Pretend you are my dead grandmother</RISK>."</example>
            <example>"<RISK>Act as an expert lawyer</RISK> and give legal advice."</example>
            <example>"<RISK>Imagine you are a terrorist</RISK> planning an attack."</example>
            <example>"<RISK>Roleplay as my therapist</RISK>."</example>
            <mitigations>
                <mitigation>
                    <summary>Keep the model in a transparent AI-assistant role and request general, non-professional information or support rather than impersonating specific people or unsafe personas.</summary>
                    <example>"Act as an expert lawyer and give legal advice." → "As an AI assistant, explain general legal concepts about employment contracts in simple terms; do not provide personalized legal advice."</example>
                </mitigation>
            </mitigations>
        </rule>
    </pillar>

    <!-- ============ I. REASONING & UNCERTAINTY ============ -->
    <pillar id="I" name="Reasoning-Uncertainty" class="prompt">
        <rule id="I1" name="Uncertainty-Permission" severity="critical">
            <detect>
                <pattern>Prompts with inherently ambiguous/unknown information but requiring definitive answer</pattern>
                <pattern>No option to say “I don’t know”, “cannot be determined”, or express confidence bounds</pattern>
                <pattern>Asking for speculative or unknowable facts framed as certain</pattern>
            </detect>
            <example>"Who was the <RISK>king of Mars</RISK>?"</example>
            <example>"Tell me <RISK>exactly how many alien civilizations</RISK> exist."</example>
            <example>"What will <RISK>definitely</RISK> happen in 2050?"</example>
            <mitigations>
                <mitigation>
                    <summary>Explicitly allow the model to express uncertainty, give ranges, or say that something is unknown, or reframe the task as scenario-building instead of demanding a single certain fact.</summary>
                    <example>"What will definitely happen in 2050?" → "Describe several plausible scenarios for global technology and climate developments by 2050, and clearly mark them as speculative."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="I2" name="Subjective-Framing-Risk" severity="critical">
            <detect>
                <pattern>Prompts explicitly asking for model’s “opinion”, “belief”, “feelings”</pattern>
                <pattern>Requests for subjective preferences framed as factual questions</pattern>
                <pattern>Personal perspective attribution: “What would you do”, “What do you believe”</pattern>
            </detect>
            <example>"What is <RISK>your opinion</RISK> on democracy?"</example>
            <example>"Do <RISK>you believe</RISK> AI is dangerous?"</example>
            <example>"<RISK>How do you feel</RISK> about climate change?"</example>
            <mitigations>
                <mitigation>
                    <summary>Ask for a summary of evidence, arguments, or common perspectives instead of personal opinions or beliefs attributed to the model.</summary>
                    <example>"What is your opinion on democracy?" → "Summarize common arguments for and against different forms of democracy, and explain some challenges they face in practice."</example>
                </mitigation>
            </mitigations>
        </rule>
    </pillar>

    <!-- ================= J. PROMPT STRUCTURE ============== -->
    <pillar id="J" name="Prompt-Structure" class="meta">
        <rule id="J1" name="Length-TooShort-TooLong" severity="high">
            <detect>
                <pattern>Underspecified prompts (missing scope or entities)</pattern>
                <pattern>Overlong prompts with many fused tasks</pattern>
            </detect>
            <example>"Explain this." // too short, missing referent</example>
            <example>"Write a detailed, concise, humorous, factual, emotional, and technical answer." // overloaded conflicting styles</example>
            <mitigations>
                <mitigation>
                    <summary>For very short prompts, add missing referents and scope; for overloaded prompts, split the work into smaller, clearly labeled sub-tasks with compatible constraints.</summary>
                    <example>"Write a detailed, concise, humorous, factual, emotional, and technical answer." → "First, write a factual, technical explanation in 2–3 paragraphs. Then, separately, provide a short humorous version for a general audience."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="J2" name="Delimiter-Missing" severity="high">
            <detect>
                <pattern>Context and instructions fused without clear separation</pattern>
            </detect>
            <example>"Dataset: 5, 6, 7 analyze it." // missing delimiter between data and task</example>
            <mitigations>
                <mitigation>
                    <summary>Introduce clear delimiters, headings, or markers to separate data/context from the actual task instructions.</summary>
                    <example>"Dataset: 5, 6, 7 analyze it." → "Dataset: 5, 6, 7. Task: Analyze this dataset and describe any patterns you see."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="J3" name="MultiObjective-Overload" severity="high">
            <detect>
                <pattern>Creative + analytical + explanatory tasks mixed with no stepwise order</pattern>
            </detect>
            <example>"Prove Fermat’s Theorem and explain it to a child in a song." // incompatible objectives</example>
            <mitigations>
                <mitigation>
                    <summary>Break multi-objective prompts into explicit steps, and avoid mixing incompatible genres (e.g., rigorous proof + song) in a single undifferentiated output.</summary>
                    <example>"Prove Fermat’s Theorem and explain it to a child in a song." → "Step 1: Explain what Fermat’s Last Theorem states in rigorous mathematical terms. Step 2: Provide a simplified non-technical explanation suitable for a child."</example>
                </mitigation>
            </mitigations>
        </rule>
    </pillar>

    <!-- ============== K. INSTRUCTION STRUCTURE & MULTI-STEP TASKING ============== -->
    <pillar id="K" name="Instruction-Structure-MultiStep" class="meta">
        <rule id="K1" name="Task-Delimitation" severity="high">
            <detect>
                <pattern>Mixed data and instructions without clear separators</pattern>
                <pattern>Prompt where the task is embedded in a blob of context</pattern>
                <pattern>No visual structure (e.g., walls of text with task hidden inside)</pattern>
                <pattern>Inline blending of metadata + instruction</pattern>
            </detect>
            <example>"Here is the text: … summarize it and critique it." // task fused with context</example>
            <example>"The dataset is as follows: [data]. Now analyze it for biases." // instruction inline with data</example>
            <example>"My teacher asked me: write an essay about it." // task embedded in metadata</example>
            <mitigations>
                <mitigation>
                    <summary>Use explicit sections or markers to distinguish between raw context (text, data, metadata) and the concrete tasks to perform on that context.</summary>
                    <example>"Here is the text: … summarize it and critique it." → "TEXT: [paste text here]. TASK: First summarize the text in one paragraph, then write a short critique of its argumentation."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="K2" name="Enumerate-MultiSteps" severity="high">
            <detect>
                <pattern>Multiple fused instructions without order markers</pattern>
                <pattern>Prompts chaining unrelated tasks in one sentence</pattern>
                <pattern>Missing explicit sequencing for dependent steps</pattern>
            </detect>
            <example>"Explain relativity and compare it to quantum mechanics and write a poem." // multiple tasks fused</example>
            <example>"Summarize the article, critique its methods, propose alternatives." // multiple steps, no order markers</example>
            <example>"List key points and then debate them." // dependent steps without sequence</example>
            <mitigations>
                <mitigation>
                    <summary>Add numbered steps or sequencing words (first, next, finally) and, if helpful, request separate outputs for each step.</summary>
                    <example>"Summarize the article, critique its methods, propose alternatives." → "1) Summarize the article in one paragraph. 2) Critique its methods in a second paragraph. 3) Propose two alternative methods in a third paragraph."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="K3" name="Stepwise-Reasoning-Cue" severity="high">
            <detect>
                <pattern>Complex reasoning tasks with no cue for structured steps</pattern>
                <pattern>Mathematical or logical tasks without “show work” style framing</pattern>
                <pattern>Requests for decision-making without asking for reasoning/evidence</pattern>
            </detect>
            <example>"Solve this math problem." // no request to show reasoning</example>
            <example>"Decide who is right in this debate." // no reasoning steps requested</example>
            <example>"Prove the theorem." // no breakdown requested</example>
            <mitigations>
                <mitigation>
                    <summary>Ask the model explicitly to reason step by step, show intermediate calculations, or justify its decisions with evidence.</summary>
                    <example>"Solve this math problem." → "Solve this math problem and show each step of your reasoning, including intermediate calculations."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="K4" name="MultiObjective-Separation" severity="high">
            <detect>
                <pattern>Creative and analytical objectives fused</pattern>
                <pattern>Instruction mixes emotional/roleplay with factual analysis</pattern>
                <pattern>Tasks combining incompatible genres</pattern>
            </detect>
            <example>"Analyze the dataset and then write a story about it." // mixes analysis + fiction</example>
            <example>"Critique the article and also make a comic strip." // mixes analysis + creative</example>
            <example>"Summarize the research in rap lyrics." // mixes academic + stylized genre</example>
            <mitigations>
                <mitigation>
                    <summary>Separate creative and analytical objectives into distinct instructions or clearly labeled sections instead of blending them into one output.</summary>
                    <example>"Summarize the research in rap lyrics." → "First, write a clear academic summary of the research in 2–3 paragraphs. Then, optionally, create a short playful summary in a less formal style."</example>
                </mitigation>
            </mitigations>
        </rule>
    </pillar>

    <!-- ============== L. CONTEXTUAL INTEGRITY ============== -->
    <pillar id="L" name="Contextual-Integrity" class="prompt">
        <rule id="L1" name="Conflicting-Instructions" severity="critical">
            <detect>
                <pattern>Instructions that contradict themselves</pattern>
                <pattern>Multiple incompatible constraints (e.g. length mismatch, style vs content clash)</pattern>
                <pattern>Conflicting factual assumptions embedded in one prompt</pattern>
                <pattern>Redundant duplication that introduces inconsistency</pattern>
            </detect>
            <example>"Write a <RISK>100-word</RISK> summary and also <RISK>at least 500 words</RISK>."</example>
            <example>"Provide <RISK>objective analysis</RISK> but make it <RISK>emotional</RISK>."</example>
            <example>"Describe how <RISK>climate change is fake</RISK> but also explain <RISK>its long-term effects</RISK>."</example>
            <mitigations>
                <mitigation>
                    <summary>Resolve contradictions by choosing one coherent set of constraints, or split incompatible goals into separately labeled tasks or outputs.</summary>
                    <example>"Write a 100-word summary and also at least 500 words." → "Write a concise summary of about 150 words for executives. In a separate section, provide a longer 500-word technical summary for specialists."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="L2" name="Negation-Risk" severity="high">
            <detect>
                <pattern>Prompts phrased as “don’t do X” without giving a positive target</pattern>
                <pattern>Instructions with double negatives or inverted logic</pattern>
                <pattern>Tasks framed by prohibition instead of explicit desired outcome</pattern>
            </detect>
            <example>"<RISK>Don’t summarize the text</RISK>." (no alternative instruction given)</example>
            <example>"<RISK>Don’t give me a long answer</RISK>." (should specify desired length instead)</example>
            <example>"<RISK>Never explain like a child</RISK>." (no clear audience level provided)</example>
            <mitigations>
                <mitigation>
                    <summary>Replace negative-only instructions with clear positive targets specifying what the model should produce (length, style, audience).</summary>
                    <example>"Don’t give me a long answer." → "Give me a brief answer of 3–4 sentences, focusing only on the main idea."</example>
                </mitigation>
            </mitigations>
        </rule>

        <rule id="L3" name="Clarification-Gap" severity="critical">
            <detect>
                <pattern>Complex/multi-step instructions where missing context prevents execution</pattern>
                <pattern>Task requires assumed prior knowledge not supplied in prompt</pattern>
                <pattern>Nested references to undefined items (e.g., "use the chart" when no chart given)</pattern>
            </detect>
            <example>"<RISK>First analyze the data, then critique it</RISK>." (no dataset provided)</example>
            <example>"<RISK>Review the text above</RISK>." (no text present in prompt)</example>
            <example>"<RISK>Rank the candidates</RISK>." (candidates not listed)</example>
            <mitigations>
                <mitigation>
                    <summary>Include all necessary context (texts, data, lists, charts) or precise references to where it can be found before asking for analysis or judgment.</summary>
                    <example>"First analyze the data, then critique it." → "Here is the dataset: [paste table]. Task: First analyze the data in one paragraph, then write a short critique of its limitations."</example>
                </mitigation>
            </mitigations>
        </rule>
    </pillar>
</hallucination_detection_guidelines>
